{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e86071",
   "metadata": {},
   "source": [
    "# NetML Challenge - Malware Detection of IoT\n",
    "\n",
    "#### Author: Robert Xing | CNetID: rkxing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b556841",
   "metadata": {},
   "source": [
    "First, we will load the training data to prepare for training our classifiers on the top-level set first (easy classes). \n",
    "\n",
    "The dataset, annotation set (labels), and helper script are provided by the NetML Challenge: https://github.com/ACANETS/NetML-Competition2020/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cfdd7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training set ...\n",
      "Reading 2_training_set.json.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(387268, 387268, 2, 387268)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.helper import get_training_data\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "training_set_folder = DATA_PATH + 'training_set'\n",
    "training_anno_file_top = DATA_PATH + 'training_anno/' + '2_training_anno_top.json.gz'\n",
    "\n",
    "# Get training data in np.array format\n",
    "Xtrain, ytrain, class_label_pair, Xtrain_ids = get_training_data(training_set_folder, training_anno_file_top)\n",
    "\n",
    "len(Xtrain), len(ytrain), len(class_label_pair), len(Xtrain_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c79864",
   "metadata": {},
   "source": [
    "Now it is time to split this training set further so that we can benchmark our classifiers.\n",
    "\n",
    "We also want to do a sanity check on our split to make sure the dimensions make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26709c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309814, 77454, 309814, 77454)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xtrain, ytrain,\n",
    "                                                test_size=TEST_SIZE,\n",
    "                                                random_state=RANDOM_STATE,\n",
    "                                                stratify=ytrain)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f358db",
   "metadata": {},
   "source": [
    "Following the feedback I received on my proposal, we need to make sure that the data is preprocessed in the same way that the original challenge expects, so we follow their method using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d80e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52beab7e",
   "metadata": {},
   "source": [
    "Now we are ready to test our first classifier. First, I've chosen to try a Random Forest Classifier, as it has performed well on similar classification problems in the past.\n",
    "\n",
    "We also use RandomizedSearchCV for hyperparameter tuning so we can ensure that we get the best results. We would prefer to use GridSearchCV, but due to the scale of the dataset, we've opted for RandomizedSearch for computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d330b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'n_estimators': 100, 'min_samples_split': 4, 'max_features': 'sqrt', 'max_depth': 20}\n",
      "Random Forest Best Estimator: RandomForestClassifier(max_depth=20, min_samples_split=4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "param_space = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "# n_jobs = -1 to use all available CPU cores\n",
    "rf_clf = RandomizedSearchCV(rf, param_space, n_jobs=-1, cv=3)\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Random Forest Best Parameters:\", rf_clf.best_params_)\n",
    "print(\"Random Forest Best Estimator:\", rf_clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e1615",
   "metadata": {},
   "source": [
    "Now we collect the metric we want to use for benchmarking against nPrint, which is the Balanced Accuracy Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df962c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Balanced Accuracy: 0.9966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "b_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Random Forest Balanced Accuracy: {b_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e8bcd",
   "metadata": {},
   "source": [
    "This is already a pretty good accuracy score, but for the sake of experimentation we'll try some others as well.\n",
    "\n",
    "First, however, we need to do another test-train split in order to avoid potential information leakages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0986152f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309814, 77454, 309814, 77454)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE += 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xtrain, ytrain,\n",
    "                                                test_size=TEST_SIZE,\n",
    "                                                random_state=RANDOM_STATE,\n",
    "                                                stratify=ytrain)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c42af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21e79e",
   "metadata": {},
   "source": [
    "For the next one, I've elected to use a Gaussian Naive Bayes Classifier, since this model should be fast and efficient.\n",
    "\n",
    "Again, we will tune to find the best hyperparameters, but here we will use GridSearchCV, since Naive Bayes is fast to fit anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a509e0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Best Parameters: {'var_smoothing': 1e-09}\n",
      "GaussianNB Best Estimator: GaussianNB()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "param_space = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "}\n",
    "\n",
    "# n_jobs = -1 to use all available CPU cores\n",
    "gnb_clf = GridSearchCV(gnb, param_space, n_jobs=-1, cv=3)\n",
    "gnb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"GaussianNB Best Parameters:\", gnb_clf.best_params_)\n",
    "print(\"GaussianNB Best Estimator:\", gnb_clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b63bb941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Balanced Accuracy: 0.6640\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb_clf.predict(X_test_scaled)\n",
    "b_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'GaussianNB Balanced Accuracy: {b_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b863e3",
   "metadata": {},
   "source": [
    "This performance is clearly much worse than our Random Forest, which we expected anyways from such a simple model, but we'll try one more.\n",
    "\n",
    "Finally, for our last model experiment we'll try a Stochastic Gradient Descent Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa729d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309814, 77454, 309814, 77454)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again doing another split for valid comparison\n",
    "\n",
    "RANDOM_STATE += 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xtrain, ytrain,\n",
    "                                                test_size=TEST_SIZE,\n",
    "                                                random_state=RANDOM_STATE,\n",
    "                                                stratify=ytrain)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2827d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b9ec2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Best Parameters: {'alpha': 1e-05, 'loss': 'hinge', 'max_iter': 1500}\n",
      "SGD Best Estimator: SGDClassifier(alpha=1e-05, max_iter=1500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "param_space = {\n",
    "    'alpha': [1e-6, 1e-5, 0.0001, 0.001, 0.01, 0.1],\n",
    "    'loss': ['hinge', 'log_loss'],\n",
    "    'max_iter': [1000, 1500, 2000],\n",
    "}\n",
    "\n",
    "sgd_clf = GridSearchCV(sgd, param_space, n_jobs=-1, cv=3)\n",
    "sgd_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"SGD Best Parameters:\", sgd_clf.best_params_)\n",
    "print(\"SGD Best Estimator:\", sgd_clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "493bac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Balanced Accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "y_pred = sgd_clf.predict(X_test_scaled)\n",
    "b_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'SGD Balanced Accuracy: {b_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e25af",
   "metadata": {},
   "source": [
    "From these trials, it still seems that our Random Forest Classifier with parameters `{'n_estimators': 100, 'min_samples_split': 4, 'max_features': 'sqrt', 'max_depth': 20}` is our highest-performing model.\n",
    "\n",
    "Now that we've found our best classifier on the top-level classes, let's try training it on the fine-level classes as well (hard set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82291742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training set ...\n",
      "Reading 2_training_set.json.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(309814, 77454, 309814, 77454)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_folder = DATA_PATH + 'training_set'\n",
    "training_anno_file_fine = DATA_PATH + 'training_anno/' + '2_training_anno_fine.json.gz'\n",
    "\n",
    "Xtrain, ytrain, class_label_pair, Xtrain_ids = get_training_data(training_set_folder, training_anno_file_fine)\n",
    "\n",
    "RANDOM_STATE += 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xtrain, ytrain,\n",
    "                                                test_size=TEST_SIZE,\n",
    "                                                random_state=RANDOM_STATE,\n",
    "                                                stratify=ytrain)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b1d925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4351bdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Balanced Accuracy (Fine): 0.5718\n"
     ]
    }
   ],
   "source": [
    "# reuse best model and parameters\n",
    "rf_clf_fine = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=4, max_features='sqrt')\n",
    "\n",
    "rf_clf_fine.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = rf_clf_fine.predict(X_test_scaled)\n",
    "\n",
    "b_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f'Random Forest Balanced Accuracy (Fine): {b_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac600d8b",
   "metadata": {},
   "source": [
    "Since we observe pretty significant performance degradation on the fine-level set, we'll first try another cross-validation to see if we can find better hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabac260",
   "metadata": {},
   "source": [
    "***Note that the following cell may take a while to run***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0564d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xingr21/cs25422/venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Fine) Best Parameters: {'n_estimators': 500, 'min_samples_split': 16, 'max_features': 'sqrt', 'max_depth': 20}\n",
      "Random Forest (Fine) Best Estimator: RandomForestClassifier(max_depth=20, min_samples_split=16, n_estimators=500)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "param_space = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 20, 30, 40],\n",
    "    'min_samples_split': [4, 8, 16],\n",
    "    'max_features': ['sqrt'],\n",
    "}\n",
    "\n",
    "rf_clf_fine = RandomizedSearchCV(rf, param_space, n_jobs=-1, cv=5)\n",
    "rf_clf_fine.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Random Forest (Fine) Best Parameters:\", rf_clf_fine.best_params_)\n",
    "print(\"Random Forest (Fine) Best Estimator:\", rf_clf_fine.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a77526f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Fine) Balanced Accuracy: 0.5726\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_clf_fine.predict(X_test_scaled)\n",
    "b_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Random Forest (Fine) Balanced Accuracy: {b_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8cffe",
   "metadata": {},
   "source": [
    "Unfortunately, it seems that even after tuning hyperparameters extensively, we are unable to reach a better accuracy with Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c904e1",
   "metadata": {},
   "source": [
    "Finally, it's time to test our model on the test and challenge sets from the NetML Challenge and find our best results for both classes of problems.\n",
    "\n",
    "First, let's test on the top-level classes sets, for which we expect good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd0d1cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training set ...\n",
      "Reading 1_test-std_set.json.gz\n",
      "Balanced Accuracy (std, Top): 0.9837\n"
     ]
    }
   ],
   "source": [
    "test_std_folder = DATA_PATH + 'test-std_set'\n",
    "test_std_anno_file_top = DATA_PATH + 'test-std_anno/' + 'top_submission_test-std_anno.json.gz'\n",
    "\n",
    "# reuse get_training_data() function to load test-std data\n",
    "X_test, y_test, _, _ = get_training_data(test_std_folder, test_std_anno_file_top)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "b_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Balanced Accuracy (std, Top): {b_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26b1e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training set ...\n",
      "Reading 0_test-challenge_set.json.gz\n",
      "Balanced Accuracy (challenge, Top): 0.9851\n"
     ]
    }
   ],
   "source": [
    "test_challenge_folder = DATA_PATH + 'test-challenge_set'\n",
    "test_challenge_anno_file_top = DATA_PATH + 'test-challenge_anno/' + 'top_submission_test-challenge_anno.json.gz'\n",
    "\n",
    "X_test, y_test, _, _ = get_training_data(test_challenge_folder, test_challenge_anno_file_top)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "b_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Balanced Accuracy (challenge, Top): {b_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b683877d",
   "metadata": {},
   "source": [
    "Next, we test again on the fine-level sets using the `rf_clf_fine` model we trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75b33227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training set ...\n",
      "Reading 1_test-std_set.json.gz\n",
      "Balanced Accuracy (std, Fine): 0.4883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xingr21/cs25422/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "test_std_anno_file_fine = DATA_PATH + 'test-std_anno/' + 'fine_submission_test-std_anno.json.gz'\n",
    "\n",
    "X_test, y_test, _, _ = get_training_data(test_std_folder, test_std_anno_file_fine)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "y_pred = rf_clf_fine.predict(X_test_scaled)\n",
    "b_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Balanced Accuracy (std, Fine): {b_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8dee4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training set ...\n",
      "Reading 0_test-challenge_set.json.gz\n",
      "Balanced Accuracy (challenge, Fine): 0.4619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xingr21/cs25422/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "test_challenge_anno_file_fine = DATA_PATH + 'test-challenge_anno/' + 'fine_submission_test-challenge_anno.json.gz'\n",
    "\n",
    "X_test, y_test, _, _ = get_training_data(test_challenge_folder, test_challenge_anno_file_fine)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "y_pred = rf_clf_fine.predict(X_test_scaled)\n",
    "b_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Balanced Accuracy (challenge, Fine): {b_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdffa56a",
   "metadata": {},
   "source": [
    "As expected, our performance on the fine-level sets was rather poor, but we were still able to achieve a high balanced accuracy on the top-level sets, beating the nPrintML benchmark of 92.4 that we originally aimed for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2fe874",
   "metadata": {},
   "source": [
    "Averaging our performance on the top-level sets, we get a combined validation accuracy of 98.6, around 6 points higher than nPrintML's best result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
